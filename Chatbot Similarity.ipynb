{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f41ea2d-9f3f-4a6c-9099-83633cc08cf7",
   "metadata": {},
   "source": [
    "### Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "303be0ef-187b-40a8-8acc-98d91ca0704c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 18:08:20,559 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2024-06-18 18:08:20,806 - INFO - Use pytorch device_name: cpu\n",
      "2024-06-18 18:08:20,807 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n",
      "2024-06-18 18:08:23,152 - INFO - Pipeline started.\n",
      "2024-06-18 18:08:26,310 - INFO - Data successfully retrieved from BigQuery.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a25374ca754643bf0ac0236661a093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 18:08:47,988 - INFO - Embeddings generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cluster                                           Question  \\\n",
      "0          36                                      Good mormning   \n",
      "1          36  Good start, but this is not what I was looking...   \n",
      "2          36                                I think it's wrong.   \n",
      "3          36                       this is not the correct one.   \n",
      "4          36                          thatâ€™s incorrect feedback   \n",
      "...       ...                                                ...   \n",
      "3918      389                                     explain E-Line   \n",
      "3919      389      under what architecture does the e-line work?   \n",
      "3920      275          how do i turn up new routers step by step   \n",
      "3921      275  how do i turn up new routers step by step for ...   \n",
      "3922      275  can you please provide a step by step guide on...   \n",
      "\n",
      "      Sentence Count  Topic  \n",
      "0                  7  False  \n",
      "1                  7  False  \n",
      "2                  7  False  \n",
      "3                  7  False  \n",
      "4                  7  False  \n",
      "...              ...    ...  \n",
      "3918               2   True  \n",
      "3919               2  False  \n",
      "3920               3   True  \n",
      "3921               3  False  \n",
      "3922               3  False  \n",
      "\n",
      "[3923 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 18:08:52,681 - INFO - Header updated in Google Sheet.\n",
      "2024-06-18 18:08:54,588 - INFO - 15692 cells updated in Google Sheet.\n",
      "2024-06-18 18:08:54,590 - INFO - DataFrame prepared for Google Sheet update.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets.readonly\",\n",
    "    \"https://www.googleapis.com/auth/bigquery\"\n",
    "]\n",
    "\n",
    "# Google Sheet IDs\n",
    "SPREADSHEET_ID = '1gq3blPuyAGbPEqc-VjW78S-J5lAJpDFAvWZo1j4ZuYY'\n",
    "SHEET_NAME = 'Results'\n",
    "PROJECT_ID = 'pccw-internal-virtual-agent'\n",
    "DATASET_ID = 'prod_wanda_ds'\n",
    "TABLE_ID = 'dialogflow_bigquery_export_data'\n",
    "\n",
    "class TextClusteringPipeline:\n",
    "    def __init__(self, spreadsheet_id: str, sheet_name: str, project_id: str, dataset_id: str, table_id: str):\n",
    "        self.spreadsheet_id = spreadsheet_id\n",
    "        self.sheet_name = sheet_name\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.table_id = table_id\n",
    "        self.creds = self.authenticate_google_api()\n",
    "        self.service = build('sheets', 'v4', credentials=self.creds)\n",
    "        self.sheet = self.service.spreadsheets()\n",
    "        self.client = bigquery.Client(credentials=self.creds, project=self.project_id)\n",
    "        self.model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "    def authenticate_google_api(self) -> Optional[Credentials]:\n",
    "        creds = None\n",
    "        if os.path.exists('token.json'):\n",
    "            creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                try:\n",
    "                    creds.refresh(Request())\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error refreshing credentials: {e}\")\n",
    "                    return None\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "                with open('token.json', 'w') as token:\n",
    "                    token.write(creds.to_json())\n",
    "        return creds\n",
    "\n",
    "    def get_data_from_bigquery(self, query: str) -> pd.DataFrame:\n",
    "        try:\n",
    "            job = self.client.query(query)\n",
    "            result = job.result().to_dataframe()\n",
    "            logging.info(\"Data successfully retrieved from BigQuery.\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to retrieve data from BigQuery: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def update_google_sheet(self, data: pd.DataFrame):\n",
    "        if data.empty:\n",
    "            logging.error(\"Received empty DataFrame, aborting Google Sheet update.\")\n",
    "            return\n",
    "        try:\n",
    "            header_values = [[\"Cluster\", \"Question\", \"Sentence Count\", \"Topic\"]]\n",
    "            self.sheet.values().update(\n",
    "                spreadsheetId=self.spreadsheet_id, range=f'{self.sheet_name}!A1',\n",
    "                valueInputOption='RAW', body={'values': header_values}\n",
    "            ).execute()\n",
    "            logging.info(\"Header updated in Google Sheet.\")\n",
    "\n",
    "            update_values = data[[\"Cluster\", \"Question\", \"Sentence Count\", \"Topic\"]].values.tolist()\n",
    "            result = self.sheet.values().update(\n",
    "                spreadsheetId=self.spreadsheet_id, range=f'{self.sheet_name}!A2',\n",
    "                valueInputOption='RAW', body={'values': update_values}\n",
    "            ).execute()\n",
    "            logging.info(f\"{result.get('updatedCells')} cells updated in Google Sheet.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to update Google Sheet: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        patterns = [\n",
    "            (r'<@U\\w+>', ''),\n",
    "            (r'[^\\w]ticket[\\w]id[\\w]redacted', 'Ticket Number'),\n",
    "            (r'INC00\\w+', 'Ticket Number'),\n",
    "            (r'(?i)console connect', ''),\n",
    "            (r'(?i)wanda', 'Chatbot'),\n",
    "            (r'(?i)SR\\d+', 'Circuit Number')\n",
    "        ]\n",
    "        for pattern, replacement in patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        return text\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Pipeline started.\")\n",
    "        query = \"\"\"\n",
    "                SELECT JSON_VALUE(request, '$.queryInput.text.text') AS Request\n",
    "                FROM `pccw-internal-virtual-agent.prod_wanda_ds.dialogflow_bigquery_export_data`\n",
    "                ORDER BY conversation_name ASC, request_time ASC\n",
    "                \"\"\"\n",
    "        data = self.get_data_from_bigquery(query)\n",
    "        if data.empty:\n",
    "            logging.error(\"No data retrieved, aborting pipeline.\")\n",
    "            return\n",
    "\n",
    "        clean_text = [self.clean_text(text) for text in data['Request'].tolist()]\n",
    "\n",
    "        corpus_embeddings = self.model.encode(clean_text)\n",
    "        logging.info(\"Embeddings generated.\")\n",
    "        \n",
    "        # Perform agglomerative clustering\n",
    "        clustering_model = AgglomerativeClustering(\n",
    "            n_clusters=None, distance_threshold=1.5\n",
    "        )  # , affinity='cosine', linkage='average', distance_threshold=0.4)\n",
    "        clustering_model.fit(corpus_embeddings)\n",
    "        cluster_assignment = clustering_model.labels_\n",
    "        \n",
    "        clustered_sentences = {}\n",
    "        for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "            if cluster_id not in clustered_sentences:\n",
    "                clustered_sentences[cluster_id] = []\n",
    "        \n",
    "            clustered_sentences[cluster_id].append(clean_text[sentence_id])\n",
    "\n",
    "        # Find the key sentence for each cluster\n",
    "        key_sentences = {}\n",
    "        for cluster_id, sentences in clustered_sentences.items():\n",
    "            cluster_indices = np.where([cluster_assignment[i] == cluster_id for i in range(len(cluster_assignment))])[0]\n",
    "            cluster_embeddings = corpus_embeddings[cluster_indices]\n",
    "            cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "            distances = np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1)\n",
    "            key_sentence_index = np.argmin(distances)\n",
    "            key_sentences[cluster_id] = clean_text[cluster_indices[key_sentence_index]]\n",
    "\n",
    "        # Count the number of sentences in each cluster\n",
    "        cluster_sentence_counts = {cluster_id: len(sentences) for cluster_id, sentences in clustered_sentences.items()}\n",
    "        \n",
    "        data = []\n",
    "        for cluster_id, sentences in clustered_sentences.items():\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                data.append({\n",
    "                    \"Cluster\": cluster_id + 1,\n",
    "                    \"Question\": sentence,\n",
    "                    \"Sentence Count\": cluster_sentence_counts[cluster_id],\n",
    "                    \"Topic\": sentence == key_sentences[cluster_id]\n",
    "                })\n",
    "                \n",
    "        df = pd.DataFrame(data)\n",
    "        self.update_google_sheet(df.sort_values(by=['Cluster']))\n",
    "        logging.info(\"DataFrame prepared for Google Sheet update.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = TextClusteringPipeline(SPREADSHEET_ID, SHEET_NAME, PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc10b6-1ad8-4077-8e4f-96e2ed25b668",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe9af8-7a97-4eb4-af1c-1d12bcd77b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
